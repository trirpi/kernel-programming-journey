{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMZqKvlvnwOnsBE2SF2FaBJ"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "05iAa_CdSHXb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2432fe9-cd2a-4363-8723-983eb90e4323"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ninja\n",
            "  Downloading ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.0 kB)\n",
            "Downloading ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/422.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m419.8/422.8 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m422.8/422.8 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ninja\n",
            "Successfully installed ninja-1.11.1.4\n"
          ]
        }
      ],
      "source": [
        "import os, math, sys, torch, re, numpy as np\n",
        "from types import SimpleNamespace as ns\n",
        "from collections import namedtuple\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.cpp_extension import load_inline\n",
        "import torch\n",
        "!pip install ninja\n",
        "import ninja"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dim3 = namedtuple('dim3', ['x', 'y', 'z'], defaults=(1, 1))"
      ],
      "metadata": {
        "id": "uL-XzsXPSSD7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.set_printoptions(precision=2, linewidth=140)\n",
        "torch.set_printoptions(precision=2, linewidth=140, sci_mode=False)"
      ],
      "metadata": {
        "id": "SA28WRebSZlK"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_img(x, figsize=(4, 3), **kwargs):\n",
        "  plt.figure(figsize=figsize)\n",
        "  plt.axis('off')\n",
        "  if len(x.shape)==3: x = x.permute(1, 2, 0)\n",
        "  plt.imshow(x.cpu(), **kwargs)\n",
        "\n",
        "cuda_begin = r\"\"\"\n",
        "#include <torch/extension.h>\n",
        "#include <stdio.h>\n",
        "#include <c10/cuda/CUDAException.h>\n",
        "\n",
        "#define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
        "#define CHECK_CONTIGUOUS(x) TORCH_CHECK(x.is_contiguous(), #x \" must be contiguous\")\n",
        "#define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
        "#define CUDA_ERR(ans) { gpuAssert((ans), __FILE__, __LINE__); }\n",
        "inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort=true)\n",
        "{\n",
        "  if (code != cudaSuccess)\n",
        "  {\n",
        "    fprintf(stderr,\"GPUassert: %s %s %d\\n\", cudaGetErrorString(code), file, line);\n",
        "    if (abort) exit(code);\n",
        "  }\n",
        "}\n",
        "__host__ __device__ inline unsigned int cdiv(unsigned int a, unsigned int b) { return (a+b-1)/b; }\n",
        "\"\"\"\n",
        "\n",
        "def load_cuda(cuda_src, cpp_src, funcs, opt=False, verbose=False, name=None):\n",
        "  if name is None: name = funcs[0]\n",
        "  return load_inline(cuda_sources=[cuda_src], cpp_sources=[cpp_src], functions=funcs,\n",
        "                     extra_cuda_cflags=[\"-O2\"] if opt else [], verbose=verbose, name=name)\n",
        "\n",
        "def cdiv(a, b):\n",
        "  return (a+b-1)//b"
      ],
      "metadata": {
        "id": "y_WZH4NuSkja"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext wurlitzer"
      ],
      "metadata": {
        "id": "LpFeAsKTS3kx"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = '1'\n",
        "torch.manual_seed(1);"
      ],
      "metadata": {
        "id": "wWjxm2TuTZyY"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m1 = torch.rand(5120, 256)\n",
        "m1s = m1[:4]\n",
        "m2 = torch.rand(256, 5120)\n",
        "m2s = m2[:, :4]"
      ],
      "metadata": {
        "id": "9ZZRudzBThqB"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def blk_kernel2d(f, blocks, threads, *args):\n",
        "  for i0 in range(blocks.y):\n",
        "    for i1 in range(blocks.x):\n",
        "      for j0 in range(threads.y):\n",
        "        for j1 in range(threads.x):\n",
        "          f(dim3(i1, i0), dim3(j1, j0), threads, *args)"
      ],
      "metadata": {
        "id": "39eWbdgeTt_g"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sig(fname, src):\n",
        "  res = re.findall(rf\"^(.+\\s+{fname}\\(.*?\\))\\s*{{?\\s*$\", src, re.MULTILINE)\n",
        "  return res[0] + ';' if res else None"
      ],
      "metadata": {
        "id": "RJ2TlTzBUIMt"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m1c, m2c = m1.contiguous().cuda(), m2.contiguous().cuda()"
      ],
      "metadata": {
        "id": "eHp05EsnVJ73"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.zeros(5)\n",
        "b, c = a[:3], a[3:]"
      ],
      "metadata": {
        "id": "DpGFOvF6Vz4n"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "b[1] = 2\n",
        "c[0] = 6\n",
        "a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y1iSoXFhek5c",
        "outputId": "4adfa953-936c-43d4-dbbc-0bf4913ec224"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 2., 0., 6., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def blk_kernel2d_shar(f, blocks, threads, sh_sz, *args, **kwargs):\n",
        "  for i0 in range(blocks.y):\n",
        "    for i1 in range(blocks.x):\n",
        "      shared = torch.zeros(sh_sz)\n",
        "      f(dim3(i1, i0), threads, shared, *args, **kwargs)"
      ],
      "metadata": {
        "id": "zWJND9dBenvC"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def matmul_tiled_blk(blockIdx, blockDim, shared, m, n, out, h, w, k, tw):\n",
        "    shar_sz = tw*tw\n",
        "    ms, ns = shared[:shar_sz], shared[shar_sz:]\n",
        "\n",
        "    for ph in range(int(math.ceil(k/tw))): # ph is the tile idx\n",
        "      idx = ph*tw\n",
        "      for tr in range(blockDim.y):\n",
        "        for tc in range(blockDim.x):\n",
        "          r, c = blockIdx.y*blockDim.y + tr, blockIdx.x*blockDim.x + tc\n",
        "          ms[tr*tw+tc] = m[tc+idx+r*k] if r<h  and idx+tc<k else 0.\n",
        "          ns[tr*tw+tc] = n[(tr+idx)*w+c] if c<w  and idx+tr<k else 0.\n",
        "\n",
        "      for t in range(blockDim.y):\n",
        "        for tc in range(blockDim.x):\n",
        "          r, c = blockIdx.y*blockDim.y + t, blockIdx.x*blockDim.x + tc\n",
        "          for i in range(tw):\n",
        "            if r*w+c<len(out):\n",
        "               out[r*w+c] += ms[t*tw+i]*ns[tw*i+tc]\n"
      ],
      "metadata": {
        "id": "SyOQFl4kfOmH"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def matmul_2d(m, n, tw=16):\n",
        "  h, k = m.shape\n",
        "  k2, w = n.shape\n",
        "  assert k == k2, \"Size mismatch!\"\n",
        "  out = torch.zeros(h, w, dtype=m.dtype)\n",
        "  tpb = dim3(tw, tw)\n",
        "  blks = dim3(cdiv(w, tpb.x), cdiv(h, tpb.y))\n",
        "  blk_kernel2d_shar(matmul_tiled_blk, blks, tpb, tw*tw*2, m.flatten(), n.flatten(),\n",
        "                    out.flatten(), h, w, k, tw)\n",
        "  return out\n"
      ],
      "metadata": {
        "id": "4OQGCuNZZw6b"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "moa-4td15Yl_"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m1s.shape, m2.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Gpkx-VTv4H2",
        "outputId": "0b750d13-1606-44e3-8473-44a29cb8b92e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([4, 256]), torch.Size([256, 5120]))"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.isclose(matmul_2d(m1s, m2s, tw=10), m1s@m2s).all()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cqULW1RUkF4M",
        "outputId": "4cb5a1a4-86e6-475c-f137-7e8a20e3a406"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(True)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run Threads"
      ],
      "metadata": {
        "id": "1Ha5A5fA4oMY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_threads(f, blockDim, *args, **kwargs):\n",
        "  for i0 in range(blockDim.y):\n",
        "    for i1 in range(blockDim.x):\n",
        "      f(i0, i1, *args, **kwargs)"
      ],
      "metadata": {
        "id": "vuzE4X1pvp01"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def matmul_tiled_blk(blockIdx, blockDim, shared, m, n, out, h, w, k, tw):\n",
        "    shar_sz = tw*tw\n",
        "    ms, ns = shared[:shar_sz], shared[shar_sz:]\n",
        "    def fill(tr, tc, ph):\n",
        "          r, c = blockIdx.y*blockDim.y + tr, blockIdx.x*blockDim.x + tc\n",
        "          ms[tr*tw+tc] = m[tc+idx+r*k] if r<h  and idx+tc<k else 0.\n",
        "          ns[tr*tw+tc] = n[(tr+idx)*w+c] if c<w  and idx+tr<k else 0.\n",
        "\n",
        "    def dot_prod(tr, tc, ph):\n",
        "          r, c = blockIdx.y*blockDim.y + tr, blockIdx.x*blockDim.x + tc\n",
        "          for i in range(tw):\n",
        "            if r*w+c<len(out):\n",
        "               out[r*w+c] += ms[tr*tw+i]*ns[tw*i+tc]\n",
        "\n",
        "    for ph in range(int(math.ceil(k/tw))): # ph is the tile idx\n",
        "      idx = ph*tw\n",
        "      run_threads(fill, blockDim, ph)\n",
        "      run_threads(dot_prod, blockDim, ph)"
      ],
      "metadata": {
        "id": "ui5gabKD4l7B"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def matmul_2d(m, n, tw=16):\n",
        "  h, k = m.shape\n",
        "  k2, w = n.shape\n",
        "  assert k == k2, \"Size mismatch!\"\n",
        "  out = torch.zeros(h, w, dtype=m.dtype)\n",
        "  tpb = dim3(tw, tw)\n",
        "  blks = dim3(cdiv(w, tpb.x), cdiv(h, tpb.y))\n",
        "  blk_kernel2d_shar(matmul_tiled_blk, blks, tpb, tw*tw*2, m.flatten(), n.flatten(),\n",
        "                    out.flatten(), h, w, k, tw)\n",
        "  return out"
      ],
      "metadata": {
        "id": "bWn40m9y5Zd1"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OYrk19425c6V"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.isclose(matmul_2d(m1s, m2s, tw=10), m1s@m2s).all()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a1e4fb3-8a5b-43e5-c773-79a9aff3f117",
        "id": "tDxedjWu5dLS"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(True)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Python threading lib"
      ],
      "metadata": {
        "id": "ldBoTIJjTr9Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import threading\n",
        "from threading import Barrier, Thread\n",
        "from concurrent.futures import ThreadPoolExecutor"
      ],
      "metadata": {
        "id": "_amZqR2r5dRz"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def blk_kernel2d_shar(f, blocks, tpb, sh_sz, *args, **kwargs):\n",
        "  for i0 in range(blocks.y):\n",
        "    for i1 in range(blocks.x):\n",
        "      shar = torch.zeros(sh_sz)\n",
        "      syncb = Barrier(tpb.y*tpb.x)\n",
        "      threads = (\n",
        "          Thread(target=f, args=(dim3(i1 ,i0), dim3(p, o), tpb, shar, syncb, *args), kwargs=kwargs)\n",
        "          for o in range(tpb.y) for p in range(tpb.x)\n",
        "      )\n",
        "      for t in threads:\n",
        "        t.start()\n",
        "      for t in threads: t.join()\n",
        "\n",
        "\n",
        "# def blk_kernel2d_shar(f, blocks, tpb, sh_sz, *args, **kwargs):\n",
        "#     for i0 in range(blocks.y):\n",
        "#         for i1 in range(blocks.x):\n",
        "#           shar = torch.zeros(sh_sz)\n",
        "#           syncb = Barrier(tpb.y*tpb.x)\n",
        "#           threads = [\n",
        "#              Thread(target=f, args=(dim3(i1,i0), dim3(p,o), tpb, shar, syncb, *args), kwargs=kwargs)\n",
        "#                    for o in range(tpb.y) for p in range(tpb.x)\n",
        "#           ]\n",
        "#           for tr in threads: tr.start()\n",
        "#           for tr in threads: tr.join()"
      ],
      "metadata": {
        "id": "D_l8EaJYTzzL"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def matmul_tiled_blk(blockIdx, threadIdx, blockDim, shared, syncb, m, n, out, h, w, k, tw):\n",
        "  tc , tr = threadIdx.x, threadIdx.y\n",
        "  r = blockIdx.y*blockDim.y+tr\n",
        "  c = blockIdx.x*blockDim.x+tc\n",
        "\n",
        "  shar_sz = tw*tw\n",
        "  ms, ns = shared[:shar_sz], shared[shar_sz:]\n",
        "\n",
        "  p = 0.\n",
        "  for ph in range(cdiv(k, tw)):\n",
        "    ms[tr*tw+tc] = m[tc+ph*tw +r*k] if r < h and (ph*tw+tc)<k else 0.\n",
        "    ns[tr*tw+tc] = n[(tr+ph*tw)* w+c] if c < w and (ph*tw+tr)<k else 0.\n",
        "    syncb.wait()\n",
        "    for i in range(tw):\n",
        "      p += ms[tr*tw+i] * ns[tw*i + tc]\n",
        "    syncb.wait()\n",
        "\n",
        "  if (r < h and c < w): out[r*w+c] = p"
      ],
      "metadata": {
        "id": "A3kNgIRfVKzj"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def matmul_2d(m, n, tw=16):\n",
        "  h, k = m.shape\n",
        "  k2, w = n.shape\n",
        "  assert k == k2, \"Size mismatch!\"\n",
        "  out = torch.zeros(h, w, dtype=m.dtype)\n",
        "  tpb = dim3(tw, tw)\n",
        "  blks = dim3(cdiv(w, tpb.x), cdiv(h, tpb.y))\n",
        "  blk_kernel2d_shar(matmul_tiled_blk, blks, tpb, tw*tw*2, m.flatten(), n.flatten(),\n",
        "                    out.flatten(), h, w, k, tw=tw)\n",
        "  return out\n",
        "matmul_2d(m1s, m2s, tw=10)\n",
        "torch.isclose(matmul_2d(m1s, m2s, tw=10), m1s@m2s).all()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ym-KoWD9a1ra",
        "outputId": "cf765c11-9649-4e1d-e8b2-88d7be496971"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(False)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "m1s@m2s"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K8Ju6CtWbDXV",
        "outputId": "62b06ec2-9348-4fc0-8850-9970f8b43802"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[69.24, 64.11, 68.03, 63.76],\n",
              "        [65.21, 62.05, 65.01, 61.14],\n",
              "        [65.99, 65.22, 66.94, 62.76],\n",
              "        [69.27, 61.63, 63.86, 61.43]])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cuda_src = cuda_begin + r\"\"\"\n",
        "__global__ void matmul_k(float *m, float *n, float *out, int h, int w, int k, int tw) {\n",
        "  int tc = threadIdx.x, tr=threadIdx.y;\n",
        "  int r = blockIdx.y*blockDim.y+tr, c = blockIdx.x*blockDim.x+tc;\n",
        "\n",
        "  extern __shared__ float ms[];\n",
        "  float *ns = &ms[tw*tw];\n",
        "\n",
        "  float p = 0.0f;\n",
        "  for (int ph = 0; ph < cdiv(k, tw); ++ph) {\n",
        "    int idx = ph*tw;\n",
        "    ms[tr*tw+tc] = r < h && idx +tc < k ? m[tc+idx+r*k]: 0.0f;\n",
        "    ns[tr*tw+tc] = c < w && idx +tr < k ? n[(tr+idx)*w+c]: 0.0f;\n",
        "    __syncthreads();\n",
        "    for (int i = 0; i < tw; ++i)\n",
        "      p += ms[tr*tw+i] * ns[tw*i+tc];\n",
        "    __syncthreads();\n",
        "  }\n",
        "  if (r < h && c < w) out[r*w+c] = p;\n",
        "}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "NEBNDDmicsqE"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cuda_src += r\"\"\"\n",
        "torch::Tensor matmul_dyn(torch::Tensor m, torch::Tensor n) {\n",
        "  CHECK_INPUT(m); CHECK_INPUT(n);\n",
        "  int h = m.size(0), w = n.size(1), k = m.size(1);\n",
        "  TORCH_CHECK(k==n.size(0), \"Size mismatch!\");\n",
        "  auto out = torch::zeros({h, w}, m.options());\n",
        "\n",
        "  int TW = 16;\n",
        "  size_t size = TW*TW*2*sizeof(float);\n",
        "\n",
        "  dim3 tpb(TW, TW);\n",
        "  dim3 blocks(cdiv(w, tpb.x), cdiv(h, tpb.y));\n",
        "  matmul_k<<<blocks, tpb, size>>>(\n",
        "    m.data_ptr<float>(), n.data_ptr<float>(), out.data_ptr<float>(), h, w, k, TW);\n",
        "  C10_CUDA_KERNEL_LAUNCH_CHECK();\n",
        "  return out;\n",
        "}\"\"\""
      ],
      "metadata": {
        "id": "Frp68-x2icGO"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fname = \"matmul_dyn\""
      ],
      "metadata": {
        "id": "KkFVSJ6nkC0j"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cpp_src = get_sig(fname, cuda_src)"
      ],
      "metadata": {
        "id": "cVrlXx7ZkF0F"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cpp_src"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "JpbTFASfkJO1",
        "outputId": "0d8a9633-74c7-4e55-acca-594b5165d17e"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'torch::Tensor matmul_dyn(torch::Tensor m, torch::Tensor n);'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# cuda_src += \"\\n// force rebuild\"\n",
        "# import shutil\n",
        "# shutil.rmtree('/root/.cache', ignore_errors=True)\n",
        "module = load_cuda(cuda_src, cpp_src, [fname], verbose=True, opt=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LjIkfXUYkOSG",
        "outputId": "4a4fce9d-39dd-4ae5-d6c7-8ec7c0144b00"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using /root/.cache/torch_extensions/py311_cu124 as PyTorch extensions root...\n",
            "The input conditions for extension module matmul_dyn have changed. Bumping to version 1 and re-building as matmul_dyn_v1...\n",
            "Detected CUDA files, patching ldflags\n",
            "Emitting ninja build file /root/.cache/torch_extensions/py311_cu124/matmul_dyn/build.ninja...\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
            "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
            "  warnings.warn(\n",
            "Building extension module matmul_dyn_v1...\n",
            "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1/3] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=matmul_dyn_v1 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /usr/local/lib/python3.11/dist-packages/torch/include -isystem /usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.11/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.11/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -c /root/.cache/torch_extensions/py311_cu124/matmul_dyn/main.cpp -o main.o \n",
            "[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda.cuda.o.d -DTORCH_EXTENSION_NAME=matmul_dyn_v1 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /usr/local/lib/python3.11/dist-packages/torch/include -isystem /usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.11/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.11/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 --compiler-options '-fPIC' -O2 -std=c++17 -c /root/.cache/torch_extensions/py311_cu124/matmul_dyn/cuda.cu -o cuda.cuda.o \n",
            "[3/3] c++ main.o cuda.cuda.o -shared -L/usr/local/lib/python3.11/dist-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o matmul_dyn_v1.so\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading extension module matmul_dyn_v1...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.isclose(module.matmul_dyn(m1c, m2c), m1c@m2c).all()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2JDiYVikViV",
        "outputId": "9e3c48b4-3d1e-421d-8373-60bfbed08499"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(True, device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "m1c@m2c"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CU1irSskn-XZ",
        "outputId": "73b0854b-a057-4d1c-d6d2-a3763c99a3ab"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[69.24, 64.11, 68.03,  ..., 64.08, 63.17, 65.47],\n",
              "        [65.21, 62.05, 65.01,  ..., 64.15, 59.35, 62.69],\n",
              "        [65.99, 65.22, 66.94,  ..., 61.65, 59.49, 62.10],\n",
              "        ...,\n",
              "        [69.24, 66.72, 66.94,  ..., 67.49, 63.27, 64.29],\n",
              "        [70.44, 65.72, 70.58,  ..., 65.22, 63.46, 67.78],\n",
              "        [71.36, 66.22, 68.45,  ..., 62.70, 62.34, 65.90]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%timeit -n 10\n",
        "module.matmul_dyn(m1c, m2c)\n",
        "torch.cuda.synchronize()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1EcNExrOonXN",
        "outputId": "9a3bc667-4be3-410c-9c41-123ef862711e"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "28.3 ms ± 3.04 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CUDA static shared"
      ],
      "metadata": {
        "id": "PW7bd_Eg0Ej7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cuda_src = cuda_begin + r\"\"\"\n",
        "constexpr int tw = 16;\n",
        "\n",
        "__global__ void matmul_k(float *m, float *n, float *out, int h, int w, int k) {\n",
        "  __shared__ float ms[tw][tw], ns[tw][tw];\n",
        "  int tc = threadIdx.x, tr=threadIdx.y;\n",
        "  int r = blockIdx.y*blockDim.y+tr, c = blockIdx.x*blockDim.x+tc;\n",
        "\n",
        "  float p = 0.0f;\n",
        "  for (int ph = 0; ph < cdiv(k, tw); ++ph) {\n",
        "    int idx = ph*tw;\n",
        "    ms[tr][tc] = r < h && idx +tc < k ? m[tc+idx+r*k]: 0.0f;\n",
        "    ns[tr][tc] = c < w && idx +tr < k ? n[(tr+idx)*w+c]: 0.0f;\n",
        "    __syncthreads();\n",
        "    for (int i = 0; i < tw; ++i)\n",
        "      p += ms[tr][i] * ns[i][tc];\n",
        "    __syncthreads();\n",
        "  }\n",
        "  if (r < h && c < w) out[r*w+c] = p;\n",
        "}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "OoNQVl3XzYu_"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cuda_src += r\"\"\"\n",
        "torch::Tensor matmul_static(torch::Tensor m, torch::Tensor n) {\n",
        "  CHECK_INPUT(m); CHECK_INPUT(n);\n",
        "  int h = m.size(0), w = n.size(1), k = m.size(1);\n",
        "  TORCH_CHECK(k==n.size(0), \"Size mismatch!\");\n",
        "  auto out = torch::zeros({h, w}, m.options());\n",
        "\n",
        "  int TW = 16;\n",
        "  size_t size = TW*TW*2*sizeof(float);\n",
        "\n",
        "  dim3 tpb(TW, TW);\n",
        "  dim3 blocks(cdiv(w, tpb.x), cdiv(h, tpb.y));\n",
        "  matmul_k<<<blocks, tpb, size>>>(\n",
        "    m.data_ptr<float>(), n.data_ptr<float>(), out.data_ptr<float>(), h, w, k);\n",
        "  C10_CUDA_KERNEL_LAUNCH_CHECK();\n",
        "  return out;\n",
        "}\"\"\""
      ],
      "metadata": {
        "id": "hVoaywKQ0Lg0"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fname = \"matmul_static\"\n",
        "cpp_src = get_sig(fname, cuda_src)\n",
        "module = load_cuda(cuda_src, cpp_src, [fname], verbose=True, opt=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eYjgbMRx0OAM",
        "outputId": "599c6aa4-265f-4d97-e0aa-3bf388a11516"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using /root/.cache/torch_extensions/py311_cu124 as PyTorch extensions root...\n",
            "The input conditions for extension module matmul_static have changed. Bumping to version 1 and re-building as matmul_static_v1...\n",
            "Detected CUDA files, patching ldflags\n",
            "Emitting ninja build file /root/.cache/torch_extensions/py311_cu124/matmul_static/build.ninja...\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
            "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
            "  warnings.warn(\n",
            "Building extension module matmul_static_v1...\n",
            "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1/3] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=matmul_static_v1 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /usr/local/lib/python3.11/dist-packages/torch/include -isystem /usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.11/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.11/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -c /root/.cache/torch_extensions/py311_cu124/matmul_static/main.cpp -o main.o \n",
            "[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda.cuda.o.d -DTORCH_EXTENSION_NAME=matmul_static_v1 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /usr/local/lib/python3.11/dist-packages/torch/include -isystem /usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.11/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.11/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 --compiler-options '-fPIC' -O2 -std=c++17 -c /root/.cache/torch_extensions/py311_cu124/matmul_static/cuda.cu -o cuda.cuda.o \n",
            "[3/3] c++ main.o cuda.cuda.o -shared -L/usr/local/lib/python3.11/dist-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o matmul_static_v1.so\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading extension module matmul_static_v1...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%timeit -n 10\n",
        "module.matmul_static(m1c, m2c)\n",
        "torch.cuda.synchronize()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kb31o6m40VCY",
        "outputId": "314b456e-cffe-4f5c-af01-328a090ccc7e"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "23.6 ms ± 2.82 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Dc6HnPv71V_k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}